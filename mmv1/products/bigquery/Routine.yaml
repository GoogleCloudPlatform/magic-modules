# Copyright 2024 Google Inc.
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

---
name: 'Routine'
kind: 'bigquery#routine'
description: |
  A user-defined function or a stored procedure that belongs to a Dataset
references:
  guides:
    'Routines Intro': 'https://cloud.google.com/bigquery/docs/reference/rest/v2/routines'
  api: 'https://cloud.google.com/bigquery/docs/reference/rest/v2/routines'
docs:
base_url: 'projects/{{project}}/datasets/{{dataset_id}}/routines'
self_link: 'projects/{{project}}/datasets/{{dataset_id}}/routines/{{routine_id}}'
import_format:
  - 'projects/{{project}}/datasets/{{dataset_id}}/routines/{{routine_id}}'
timeouts:
  insert_minutes: 20
  update_minutes: 20
  delete_minutes: 20
custom_code:
examples:
  - name: 'bigquery_routine_basic'
    primary_resource_id: 'sproc'
    primary_resource_name: 'fmt.Sprintf("tf_test_dataset_id%s", context["random_suffix"]), fmt.Sprintf("tf_test_table_id%s", context["random_suffix"])'
    vars:
      dataset_id: 'dataset_id'
      routine_id: 'routine_id'
  - name: 'bigquery_routine_json'
    primary_resource_id: 'sproc'
    primary_resource_name: 'fmt.Sprintf("tf_test_dataset_id%s", context["random_suffix"]), fmt.Sprintf("tf_test_table_id%s", context["random_suffix"])'
    vars:
      dataset_id: 'dataset_id'
      routine_id: 'routine_id'
  - name: 'bigquery_routine_tvf'
    primary_resource_id: 'sproc'
    primary_resource_name: 'fmt.Sprintf("tf_test_dataset_id%s", context["random_suffix"]), fmt.Sprintf("tf_test_table_id%s", context["random_suffix"])'
    vars:
      dataset_id: 'dataset_id'
      routine_id: 'routine_id'
  - name: 'bigquery_routine_pyspark'
    primary_resource_id: 'pyspark'
    vars:
      dataset_id: 'dataset_id'
      connection_id: 'connection_id'
      routine_id: 'routine_id'
  - name: 'bigquery_routine_pyspark_mainfile'
    primary_resource_id: 'pyspark_mainfile'
    vars:
      dataset_id: 'dataset_id'
      connection_id: 'connection_id'
      routine_id: 'routine_id'
  - name: 'bigquery_routine_spark_jar'
    primary_resource_id: 'spark_jar'
    vars:
      dataset_id: 'dataset_id'
      connection_id: 'connection_id'
      routine_id: 'routine_id'
  - name: 'bigquery_routine_data_governance_type'
    primary_resource_id: 'custom_masking_routine'
    vars:
      dataset_id: 'dataset_id'
      routine_id: 'routine_id'
  - name: 'bigquery_routine_remote_function'
    primary_resource_id: 'remote_function'
    vars:
      dataset_id: 'dataset_id'
      connection_id: 'connection_id'
      routine_id: 'routine_id'
    exclude_test: true
parameters:
properties:
  - name: 'routineReference'
    type: NestedObject
    description: Reference describing the ID of this routine
    required: true
    custom_expand: 'templates/terraform/custom_expand/bigquery_routine_ref.go.tmpl'
    flatten_object: true
    properties:
      - name: 'datasetId'
        type: String
        description: The ID of the dataset containing this routine
        required: true
        immutable: true
      - name: 'routineId'
        type: String
        description:
          The ID of the the routine. The ID must contain only letters (a-z,
          A-Z), numbers (0-9), or underscores (_). The maximum length is 256
          characters.
        required: true
        immutable: true
  - name: 'routineType'
    type: Enum
    description: The type of routine.
    required: true
    immutable: true
    enum_values:
      - 'SCALAR_FUNCTION'
      - 'PROCEDURE'
      - 'TABLE_VALUED_FUNCTION'
  - name: 'creationTime'
    type: Integer
    description: |
      The time when this routine was created, in milliseconds since the
      epoch.
    output: true
  - name: 'lastModifiedTime'
    type: Integer
    description: |
      The time when this routine was modified, in milliseconds since the
      epoch.
    output: true
  - name: 'language'
    type: Enum
    description: |
      The language of the routine.
    enum_values:
      - 'SQL'
      - 'JAVASCRIPT'
      - 'PYTHON'
      - 'JAVA'
      - 'SCALA'
  - name: 'arguments'
    type: Array
    description: Input/output argument of a function or a stored procedure.
    item_type:
      type: NestedObject
      properties:
        - name: 'name'
          type: String
          description: |
            The name of this argument. Can be absent for function return argument.
        - name: 'argumentKind'
          type: Enum
          description: Defaults to FIXED_TYPE.
          default_value: "FIXED_TYPE"
          enum_values:
            - 'FIXED_TYPE'
            - 'ANY_TYPE'
        - name: 'mode'
          type: Enum
          description: |
            Specifies whether the argument is input or output. Can be set for procedures only.
          enum_values:
            - 'IN'
            - 'OUT'
            - 'INOUT'
        # This is a string instead of a NestedObject because schemas contain ColumnSchemas,
        # which can contain nested StandardSqlDataType.
        # We'll have people provide the json blob for the schema instead.
        - name: 'dataType'
          type: String
          description: |
            A JSON schema for the data type. Required unless argumentKind = ANY_TYPE.
            ~>**NOTE**: Because this field expects a JSON string, any changes to the string
            will create a diff, even if the JSON itself hasn't changed. If the API returns
            a different value for the same schema, e.g. it switched the order of values
            or replaced STRUCT field type with RECORD field type, we currently cannot
            suppress the recurring diff this causes. As a workaround, we recommend using
            the schema as returned by the API.
          state_func: 'func(v interface{}) string { s, _ := structure.NormalizeJsonString(v); return s }'
          custom_flatten: 'templates/terraform/custom_flatten/json_schema.tmpl'
          custom_expand: 'templates/terraform/custom_expand/json_schema.tmpl'
          validation:
            function: 'validation.StringIsJSON'
  - name: 'returnType'
    type: String
    description: |
      A JSON schema for the return type. Optional if language = "SQL"; required otherwise.
      If absent, the return type is inferred from definitionBody at query time in each query
      that references this routine. If present, then the evaluated result will be cast to
      the specified returned type at query time. ~>**NOTE**: Because this field expects a JSON
      string, any changes to the string will create a diff, even if the JSON itself hasn't
      changed. If the API returns a different value for the same schema, e.g. it switche
      d the order of values or replaced STRUCT field type with RECORD field type, we currently
      cannot suppress the recurring diff this causes. As a workaround, we recommend using
      the schema as returned by the API.
    state_func: 'func(v interface{}) string { s, _ := structure.NormalizeJsonString(v); return s }'
    custom_flatten: 'templates/terraform/custom_flatten/json_schema.tmpl'
    custom_expand: 'templates/terraform/custom_expand/json_schema.tmpl'
    validation:
      function: 'validation.StringIsJSON'
  - name: 'returnTableType'
    type: String
    description: |
      Optional. Can be set only if routineType = "TABLE_VALUED_FUNCTION".

      If absent, the return table type is inferred from definitionBody at query time in each query
      that references this routine. If present, then the columns in the evaluated table result will
      be cast to match the column types specificed in return table type, at query time.
    state_func: 'func(v interface{}) string { s, _ := structure.NormalizeJsonString(v); return s }'
    custom_flatten: 'templates/terraform/custom_flatten/json_schema.tmpl'
    custom_expand: 'templates/terraform/custom_expand/json_schema.tmpl'
    validation:
      function: 'validation.StringIsJSON'
  - name: 'importedLibraries'
    type: Array
    description: |
      Optional. If language = "JAVASCRIPT", this field stores the path of the
      imported JAVASCRIPT libraries.
    item_type:
      type: String
  - name: 'definitionBody'
    type: String
    description: |
      The body of the routine. For functions, this is the expression in the AS clause.
      If language=SQL, it is the substring inside (but excluding) the parentheses.
    required: true
  - name: 'description'
    type: String
    description: The description of the routine if defined.
  - name: 'determinismLevel'
    type: Enum
    description: The determinism level of the JavaScript UDF if defined.
    enum_values:
      - 'DETERMINISM_LEVEL_UNSPECIFIED'
      - 'DETERMINISTIC'
      - 'NOT_DETERMINISTIC'
  - name: 'dataGovernanceType'
    type: Enum
    description: If set to DATA_MASKING, the function is validated and made available as a masking function. For more information, see https://cloud.google.com/bigquery/docs/user-defined-functions#custom-mask
    enum_values:
      - 'DATA_MASKING'
  - name: 'sparkOptions'
    type: NestedObject
    description: |
      Optional. If language is one of "PYTHON", "JAVA", "SCALA", this field stores the options for spark stored procedure.
    properties:
      - name: 'connection'
        type: String
        description: |
          Fully qualified name of the user-provided Spark connection object.
          Format: "projects/{projectId}/locations/{locationId}/connections/{connectionId}"
      - name: 'runtimeVersion'
        type: String
        description: Runtime version. If not specified, the default runtime version is used.
      - name: 'containerImage'
        type: String
        description: Custom container image for the runtime environment.
      - name: 'properties'
        type: KeyValuePairs
        description: |
          Configuration properties as a set of key/value pairs, which will be passed on to the Spark application.
          For more information, see Apache Spark and the procedure option list.
          An object containing a list of "key": value pairs. Example: { "name": "wrench", "mass": "1.3kg", "count": "3" }.
        default_from_api: true
      - name: 'mainFileUri'
        type: String
        description: |
          The main file/jar URI of the Spark application.
          Exactly one of the definitionBody field and the mainFileUri field must be set for Python.
          Exactly one of mainClass and mainFileUri field should be set for Java/Scala language type.
      - name: 'pyFileUris'
        type: Array
        description: |
          Python files to be placed on the PYTHONPATH for PySpark application. Supported file types: .py, .egg, and .zip. For more information about Apache Spark, see Apache Spark.
        default_from_api: true
        item_type:
          type: String
      - name: 'jarUris'
        type: Array
        description: |
          JARs to include on the driver and executor CLASSPATH. For more information about Apache Spark, see Apache Spark.
        default_from_api: true
        item_type:
          type: String
      - name: 'fileUris'
        type: Array
        description: |
          Files to be placed in the working directory of each executor. For more information about Apache Spark, see Apache Spark.
        default_from_api: true
        item_type:
          type: String
      - name: 'archiveUris'
        type: Array
        description: |
          Archive files to be extracted into the working directory of each executor. For more information about Apache Spark, see Apache Spark.
        default_from_api: true
        item_type:
          type: String
      - name: 'mainClass'
        type: String
        description: |
          The fully qualified name of a class in jarUris, for example, com.example.wordcount.
          Exactly one of mainClass and main_jar_uri field should be set for Java/Scala language type.
  - name: 'remoteFunctionOptions'
    type: NestedObject
    description: Remote function specific options.
    properties:
      - name: 'endpoint'
        type: String
        description: |
          Endpoint of the user-provided remote service, e.g.
          `https://us-east1-my_gcf_project.cloudfunctions.net/remote_add`
      - name: 'connection'
        type: String
        description: |
          Fully qualified name of the user-provided connection object which holds
          the authentication information to send requests to the remote service.
          Format: "projects/{projectId}/locations/{locationId}/connections/{connectionId}"
      - name: 'userDefinedContext'
        type: KeyValuePairs
        description: |
          User-defined context as a set of key/value pairs, which will be sent as function
          invocation context together with batched arguments in the requests to the remote
          service. The total number of bytes of keys and values must be less than 8KB.

          An object containing a list of "key": value pairs. Example:
          `{ "name": "wrench", "mass": "1.3kg", "count": "3" }`.
        default_from_api: true
      - name: 'maxBatchingRows'
        type: String
        description: |
          Max number of rows in each batch sent to the remote service. If absent or if 0,
          BigQuery dynamically decides the number of rows in a batch.
