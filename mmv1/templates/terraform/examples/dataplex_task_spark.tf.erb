# VPC network
resource "google_compute_network" "default" {
    name                    = "tf-test-workstation-cluster%{random_suffix}"
    auto_create_subnetworks = true
}

data "google_project" "project" {

}

resource "google_dataplex_lake" "<%= ctx[:primary_resource_id] %>" {
  name         = "tf-test-lake%{random_suffix}"
  location     = "us-central1"
  project = "<%= ctx[:test_env_vars]['project_name'] %>"
}


resource "google_dataplex_task" "<%= ctx[:primary_resource_id] %>" {

    task_id      = "tf-test-task%{random_suffix}"
    location     = "us-central1"
    lake         = google_dataplex_lake.<%= ctx[:primary_resource_id] %>.name
    trigger_spec  {
        type = "ON_DEMAND"
    }
    
    description = "task-spark-terraform"

    execution_spec {
        service_account = "${data.google_project.project.number}-compute@developer.gserviceaccount.com"
        args = {
            TASK_ARGS  = "--output_location,gs://spark-job/task-result, --output_format, json"
        }

    }
    
    spark {
        infrastructure_spec  {
            batch {
                executors_count = 2
                max_executors_count = 100
            }
            container_image {
                image = "test-image"
                java_jars = ["test-java-jars.jar"]
                python_packages = ["gs://bucket-name/my/path/to/lib.tar.gz"]
                properties = { "name": "wrench", "mass": "1.3kg", "count": "3" }
            }
            vpc_network  {
                    network_tags = ["test-network-tag"]
                    sub_network = google_compute_network.default.id
                }
        }
        file_uris = ["gs://terrafrom-test/test.csv"]
        archive_uris = ["gs://terraform-test/test.csv"]
        sql_script = "show databases"
    }
    
    project = "<%= ctx[:test_env_vars]['project_name'] %>"
    
}